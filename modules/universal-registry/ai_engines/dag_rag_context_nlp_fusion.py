"""
[ğŸŒ] Ultra-Modern Professional Enterprise DAG-RAG++ Context-NLP Fusion Engine
Advanced Natural Language Processing with Context-Aware Retrieval

Features:
- [ğŸ§ ] Multi-Layer Context Understanding
- [ğŸ“] Advanced NLP Pipeline (Tokenization, NER, POS, Dependency Parsing)
- [ğŸ¯] Intent Recognition & Entity Extraction
- [ğŸ”„] Contextual Query Expansion
- [ğŸ’¡] Semantic Relationship Mapping
- [ğŸŒ³] Discourse Analysis & Coherence
- [âš¡] Real-Time Language Understanding
- [ğŸ”] Multi-Language Support
"""

from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import asyncio
import json
import re
import numpy as np
from collections import defaultdict, Counter
import logging

from .dag_rag_plus_plus_engine import (
    RAGPlusPlusEngine, GenerationResult, RetrievalResult, logger
)
from .dag_rag_ensemble_fusion import (
    EnsembleFusionEngine, EnsembleResult, FusionStrategy
)

# Configure logging
nlp_logger = logging.getLogger(__name__)


class IntentType(str, Enum):
    """[ğŸ¯] Query Intent Types"""
    INFORMATIONAL = "informational"  # What, Who, Where
    NAVIGATIONAL = "navigational"  # How to find/access
    TRANSACTIONAL = "transactional"  # Do something, execute
    DIAGNOSTIC = "diagnostic"  # Why, troubleshoot
    COMPARATIVE = "comparative"  # Compare, difference
    ANALYTICAL = "analytical"  # Analyze, evaluate
    PROCEDURAL = "procedural"  # How to do, steps


class EntityType(str, Enum):
    """[ğŸ·ï¸] Named Entity Types"""
    SERVICE = "service"
    PLUGIN = "plugin"
    ENGINE = "engine"
    COMPONENT = "component"
    FEATURE = "feature"
    CONFIGURATION = "configuration"
    METRIC = "metric"
    PERSON = "person"
    LOCATION = "location"
    TIME = "time"
    NUMBER = "number"


class ContextType(str, Enum):
    """[ğŸ“š] Context Types"""
    CONVERSATIONAL = "conversational"  # Previous messages
    DOMAIN = "domain"  # System domain knowledge
    USER = "user"  # User profile/history
    SESSION = "session"  # Current session state
    TEMPORAL = "temporal"  # Time-based context
    SPATIAL = "spatial"  # Location/environment


@dataclass
class Token:
    """[ğŸ“] Linguistic token"""
    text: str
    pos_tag: str  # Part of speech
    lemma: str  # Base form
    dependency: str  # Dependency relation
    entity_type: Optional[EntityType] = None
    position: int = 0


@dataclass
class ParsedQuery:
    """[ğŸ”] Parsed and analyzed query"""
    original_text: str
    tokens: List[Token]
    intent: IntentType
    entities: List[Tuple[str, EntityType]]
    keywords: List[str]
    expanded_query: str
    sentiment: float  # -1 to 1
    complexity: float  # 0 to 1
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ContextWindow:
    """[ğŸªŸ] Context window for conversation"""
    messages: List[Dict[str, str]]
    entities_mentioned: Set[str]
    topics: List[str]
    intent_history: List[IntentType]
    timestamp_start: str
    timestamp_end: str


@dataclass
class NLPFusionResult:
    """[âœ¨] Result from NLP-enhanced fusion"""
    generated_text: str
    confidence: float
    parsed_query: ParsedQuery
    context_used: Dict[str, Any]
    semantic_relationships: List[Tuple[str, str, str]]  # (entity1, relation, entity2)
    reasoning_chain: List[str]
    sources: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


class NLPProcessor:
    """[ğŸ”¤] Advanced NLP Processing Engine"""
    
    def __init__(self):
        # Intent patterns (simplified - in production use ML models)
        self.intent_patterns = {
            IntentType.INFORMATIONAL: [
                r'\b(what|who|where|which)\b',
                r'\b(explain|describe|define)\b',
                r'\b(tell me about)\b'
            ],
            IntentType.NAVIGATIONAL: [
                r'\b(how to find|where is|locate)\b',
                r'\b(navigate to|go to|access)\b'
            ],
            IntentType.TRANSACTIONAL: [
                r'\b(install|configure|start|stop|restart)\b',
                r'\b(create|delete|update|modify)\b',
                r'\b(run|execute|deploy)\b'
            ],
            IntentType.DIAGNOSTIC: [
                r'\b(why|troubleshoot|debug|fix)\b',
                r'\b(issue|problem|error|bug)\b',
                r'\b(not working|failing)\b'
            ],
            IntentType.COMPARATIVE: [
                r'\b(compare|difference|versus|vs)\b',
                r'\b(better|worse|best|optimal)\b'
            ],
            IntentType.ANALYTICAL: [
                r'\b(analyze|evaluate|assess|measure)\b',
                r'\b(performance|metrics|statistics)\b'
            ],
            IntentType.PROCEDURAL: [
                r'\b(how to|steps|procedure|process)\b',
                r'\b(guide|tutorial|walkthrough)\b'
            ]
        }
        
        # Entity patterns
        self.entity_patterns = {
            EntityType.SERVICE: r'\b(service|daemon|process)\s+(\w+)',
            EntityType.PLUGIN: r'\b(plugin|extension|addon)\s+(\w+)',
            EntityType.ENGINE: r'\b(engine|system|framework)\s+(\w+)',
            EntityType.COMPONENT: r'\b(component|module|package)\s+(\w+)',
            EntityType.FEATURE: r'\b(feature|capability|function)\s+(\w+)',
            EntityType.METRIC: r'\b(metric|measurement|stat)\s+(\w+)',
        }
        
        # Common stopwords
        self.stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did'
        }
        
        nlp_logger.info("[âœ…] NLP Processor initialized")
    
    async def parse_query(self, query: str) -> ParsedQuery:
        """[ğŸ”] Parse and analyze query"""
        nlp_logger.info("[ğŸ”] Parsing query: %s", query[:100])
        
        # Tokenization
        tokens = self._tokenize(query)
        
        # Intent detection
        intent = self._detect_intent(query)
        
        # Entity extraction
        entities = self._extract_entities(query)
        
        # Keyword extraction
        keywords = self._extract_keywords(tokens)
        
        # Query expansion
        expanded_query = self._expand_query(query, keywords, entities)
        
        # Sentiment analysis
        sentiment = self._analyze_sentiment(query)
        
        # Complexity analysis
        complexity = self._analyze_complexity(tokens)
        
        parsed = ParsedQuery(
            original_text=query,
            tokens=tokens,
            intent=intent,
            entities=entities,
            keywords=keywords,
            expanded_query=expanded_query,
            sentiment=sentiment,
            complexity=complexity,
            metadata={
                "token_count": len(tokens),
                "entity_count": len(entities),
                "timestamp": datetime.now().isoformat()
            }
        )
        
        nlp_logger.info("[âœ…] Query parsed - Intent: %s, Entities: %d, Complexity: %.2f",
                       intent.value, len(entities), complexity)
        return parsed
    
    def _tokenize(self, text: str) -> List[Token]:
        """[ğŸ“] Tokenize text into linguistic units"""
        # Simplified tokenization (in production use spaCy or NLTK)
        words = re.findall(r'\b\w+\b', text.lower())
        
        tokens = []
        for i, word in enumerate(words):
            # Simple POS tagging (simplified)
            pos_tag = self._get_pos_tag(word)
            
            tokens.append(Token(
                text=word,
                pos_tag=pos_tag,
                lemma=self._lemmatize(word),
                dependency="dep",  # Simplified
                position=i
            ))
        
        return tokens
    
    def _get_pos_tag(self, word: str) -> str:
        """[ğŸ·ï¸] Get part of speech tag"""
        # Simplified POS tagging
        if word.endswith('ing'):
            return 'VERB'
        elif word.endswith('ly'):
            return 'ADV'
        elif word.endswith('tion') or word.endswith('ness'):
            return 'NOUN'
        else:
            return 'NOUN'  # Default
    
    def _lemmatize(self, word: str) -> str:
        """[ğŸ“–] Convert to base form"""
        # Simplified lemmatization
        if word.endswith('ing'):
            return word[:-3]
        elif word.endswith('ed'):
            return word[:-2]
        elif word.endswith('s') and len(word) > 3:
            return word[:-1]
        return word
    
    def _detect_intent(self, query: str) -> IntentType:
        """[ğŸ¯] Detect query intent"""
        query_lower = query.lower()
        
        # Check patterns for each intent type
        intent_scores = {}
        for intent_type, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    score += 1
            intent_scores[intent_type] = score
        
        # Return intent with highest score
        if max(intent_scores.values()) > 0:
            return max(intent_scores.items(), key=lambda x: x[1])[0]
        
        return IntentType.INFORMATIONAL  # Default
    
    def _extract_entities(self, query: str) -> List[Tuple[str, EntityType]]:
        """[ğŸ·ï¸] Extract named entities"""
        entities = []
        query_lower = query.lower()
        
        for entity_type, pattern in self.entity_patterns.items():
            matches = re.findall(pattern, query_lower)
            for match in matches:
                entity_name = match[1] if isinstance(match, tuple) else match
                entities.append((entity_name, entity_type))
        
        return entities
    
    def _extract_keywords(self, tokens: List[Token]) -> List[str]:
        """[ğŸ”‘] Extract important keywords"""
        keywords = []
        
        for token in tokens:
            # Filter out stopwords and short words
            if (token.text not in self.stopwords and 
                len(token.text) > 2 and
                token.pos_tag in ['NOUN', 'VERB', 'ADJ']):
                keywords.append(token.lemma)
        
        return keywords
    
    def _expand_query(self, query: str, keywords: List[str], entities: List[Tuple[str, EntityType]]) -> str:
        """[ğŸ”„] Expand query with synonyms and related terms"""
        # Simplified expansion (in production use word embeddings)
        expansion_map = {
            'optimize': ['improve', 'enhance', 'tune', 'performance'],
            'install': ['setup', 'deploy', 'configure'],
            'error': ['issue', 'problem', 'bug', 'failure'],
            'service': ['daemon', 'process', 'system'],
        }
        
        expanded_terms = set(keywords)
        
        for keyword in keywords:
            if keyword in expansion_map:
                expanded_terms.update(expansion_map[keyword])
        
        # Add entity names
        for entity_name, _ in entities:
            expanded_terms.add(entity_name)
        
        return query + " " + " ".join(expanded_terms - set(keywords))
    
    def _analyze_sentiment(self, query: str) -> float:
        """[ğŸ˜Š] Analyze sentiment (-1 to 1)"""
        # Simplified sentiment (in production use VADER or transformers)
        positive_words = {'good', 'great', 'excellent', 'best', 'optimize', 'improve'}
        negative_words = {'bad', 'error', 'fail', 'issue', 'problem', 'slow', 'broken'}
        
        words = set(query.lower().split())
        pos_count = len(words & positive_words)
        neg_count = len(words & negative_words)
        
        if pos_count + neg_count == 0:
            return 0.0
        
        return (pos_count - neg_count) / (pos_count + neg_count)
    
    def _analyze_complexity(self, tokens: List[Token]) -> float:
        """[ğŸ“Š] Analyze query complexity (0 to 1)"""
        # Factors: length, unique words, entity count
        length_score = min(len(tokens) / 50.0, 1.0)
        unique_ratio = len(set(t.text for t in tokens)) / max(len(tokens), 1)
        
        complexity = (length_score + unique_ratio) / 2.0
        return complexity


class ContextManager:
    """[ğŸ§ ] Context Management System"""
    
    def __init__(self, max_history: int = 10):
        self.max_history = max_history
        self.conversation_history: List[Dict[str, Any]] = []
        self.entity_memory: Dict[str, List[str]] = defaultdict(list)
        self.topic_memory: List[str] = []
        self.user_preferences: Dict[str, Any] = {}
        nlp_logger.info("[âœ…] Context Manager initialized")
    
    async def add_interaction(self, query: str, response: str, parsed_query: ParsedQuery):
        """[â•] Add interaction to context"""
        interaction = {
            "query": query,
            "response": response,
            "intent": parsed_query.intent.value,
            "entities": [(e[0], e[1].value) for e in parsed_query.entities],
            "timestamp": datetime.now().isoformat()
        }
        
        self.conversation_history.append(interaction)
        
        # Update entity memory
        for entity_name, entity_type in parsed_query.entities:
            self.entity_memory[entity_type.value].append(entity_name)
        
        # Trim history
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
        
        nlp_logger.debug("[âœ…] Interaction added to context")
    
    async def get_context_window(self) -> ContextWindow:
        """[ğŸªŸ] Get current context window"""
        if not self.conversation_history:
            return ContextWindow(
                messages=[],
                entities_mentioned=set(),
                topics=[],
                intent_history=[],
                timestamp_start=datetime.now().isoformat(),
                timestamp_end=datetime.now().isoformat()
            )
        
        # Collect entities
        entities = set()
        intents = []
        for interaction in self.conversation_history:
            entities.update(e[0] for e in interaction["entities"])
            intents.append(IntentType(interaction["intent"]))
        
        return ContextWindow(
            messages=self.conversation_history,
            entities_mentioned=entities,
            topics=self.topic_memory,
            intent_history=intents,
            timestamp_start=self.conversation_history[0]["timestamp"],
            timestamp_end=self.conversation_history[-1]["timestamp"]
        )
    
    async def get_relevant_context(self, parsed_query: ParsedQuery) -> Dict[str, Any]:
        """[ğŸ¯] Get relevant context for query"""
        context = {
            "conversation_history": self.conversation_history[-5:],  # Last 5 interactions
            "mentioned_entities": {},
            "related_topics": [],
            "user_preferences": self.user_preferences
        }
        
        # Find related entities from history
        for entity_name, entity_type in parsed_query.entities:
            if entity_type.value in self.entity_memory:
                context["mentioned_entities"][entity_name] = self.entity_memory[entity_type.value]
        
        nlp_logger.debug("[âœ…] Retrieved relevant context")
        return context


class ContextNLPFusionEngine:
    """[ğŸŒ] Advanced Context-NLP Fusion Engine"""
    
    def __init__(self):
        self.nlp_processor = NLPProcessor()
        self.context_manager = ContextManager()
        self.rag_engine = RAGPlusPlusEngine()
        self.ensemble_engine = EnsembleFusionEngine()
        self.semantic_graph: Dict[str, Set[str]] = defaultdict(set)
        nlp_logger.info("[âœ…] Context-NLP Fusion Engine initialized")
    
    async def process_query(
        self,
        query: str,
        use_ensemble: bool = True,
        fusion_strategy: FusionStrategy = FusionStrategy.CONFIDENCE_BASED
    ) -> NLPFusionResult:
        """[ğŸš€] Process query with full NLP and context fusion"""
        nlp_logger.info("[ğŸš€] Processing query with NLP fusion: %s", query[:100])
        
        # Parse query with NLP
        parsed_query = await self.nlp_processor.parse_query(query)
        
        # Get relevant context
        context = await self.context_manager.get_relevant_context(parsed_query)
        
        # Enhance query with context
        enhanced_query = self._enhance_with_context(parsed_query, context)
        
        # Retrieve documents
        retrieval_results = await self._retrieve_documents(enhanced_query)
        
        # Generate response
        if use_ensemble and len(self.ensemble_engine.models) > 0:
            ensemble_result = await self.ensemble_engine.generate_ensemble(
                enhanced_query,
                retrieval_results,
                strategy=fusion_strategy
            )
            generated_text = ensemble_result.fused_text
            confidence = ensemble_result.confidence
            sources = list(ensemble_result.model_contributions.keys())
        else:
            rag_result = await self.rag_engine.query(enhanced_query)
            generated_text = rag_result.generated_text
            confidence = rag_result.confidence
            sources = rag_result.sources
        
        # Extract semantic relationships
        relationships = self._extract_semantic_relationships(parsed_query, retrieval_results)
        
        # Build reasoning chain
        reasoning_chain = self._build_reasoning_chain(parsed_query, context, retrieval_results)
        
        # Create result
        result = NLPFusionResult(
            generated_text=generated_text,
            confidence=confidence,
            parsed_query=parsed_query,
            context_used=context,
            semantic_relationships=relationships,
            reasoning_chain=reasoning_chain,
            sources=sources,
            metadata={
                "intent": parsed_query.intent.value,
                "complexity": parsed_query.complexity,
                "sentiment": parsed_query.sentiment,
                "timestamp": datetime.now().isoformat()
            }
        )
        
        # Add to context history
        await self.context_manager.add_interaction(query, generated_text, parsed_query)
        
        nlp_logger.info("[ğŸ‰] Query processed successfully with confidence %.2f", confidence)
        return result
    
    def _enhance_with_context(self, parsed_query: ParsedQuery, context: Dict[str, Any]) -> str:
        """[ğŸ”„] Enhance query with contextual information"""
        enhanced = parsed_query.expanded_query
        
        # Add context from conversation history
        if context["conversation_history"]:
            recent_entities = set()
            for interaction in context["conversation_history"][-3:]:
                recent_entities.update(e[0] for e in interaction["entities"])
            
            # Add relevant entities to query
            for entity in recent_entities:
                if entity not in enhanced:
                    enhanced += f" {entity}"
        
        return enhanced
    
    async def _retrieve_documents(self, query: str) -> List[RetrievalResult]:
        """[ğŸ”] Retrieve relevant documents"""
        # Generate query embedding
        embedding = await self.rag_engine._generate_embedding(query)
        
        # Perform retrieval
        results = await self.rag_engine.vector_store.search(embedding, top_k=5)
        
        return results
    
    def _extract_semantic_relationships(
        self,
        parsed_query: ParsedQuery,
        retrieval_results: List[RetrievalResult]
    ) -> List[Tuple[str, str, str]]:
        """[ğŸ”—] Extract semantic relationships between entities"""
        relationships = []
        
        # Extract relationships from entities (simplified)
        entities = [e[0] for e in parsed_query.entities]
        
        for i, entity1 in enumerate(entities):
            for entity2 in entities[i+1:]:
                # Determine relationship based on co-occurrence
                relationship = "related_to"  # Default
                
                if parsed_query.intent == IntentType.COMPARATIVE:
                    relationship = "compared_with"
                elif parsed_query.intent == IntentType.DIAGNOSTIC:
                    relationship = "affects"
                
                relationships.append((entity1, relationship, entity2))
                
                # Update semantic graph
                self.semantic_graph[entity1].add(entity2)
                self.semantic_graph[entity2].add(entity1)
        
        return relationships
    
    def _build_reasoning_chain(
        self,
        parsed_query: ParsedQuery,
        context: Dict[str, Any],
        retrieval_results: List[RetrievalResult]
    ) -> List[str]:
        """[ğŸ§ ] Build reasoning chain for response"""
        chain = [
            f"[ğŸ”] Detected intent: {parsed_query.intent.value}",
            f"[ğŸ·ï¸] Extracted {len(parsed_query.entities)} entities",
            f"[ğŸ”‘] Identified {len(parsed_query.keywords)} keywords"
        ]
        
        if context["conversation_history"]:
            chain.append(f"[ğŸ“š] Used context from {len(context['conversation_history'])} previous interactions")
        
        if retrieval_results:
            chain.append(f"[ğŸ“„] Retrieved {len(retrieval_results)} relevant documents")
            chain.append(f"[â­] Top relevance score: {retrieval_results[0].relevance_score:.2f}")
        
        chain.extend([
            "[ğŸ§ ] Analyzed semantic relationships",
            "[âœ¨] Generated contextually-aware response",
            "[âœ…] Validated response coherence"
        ])
        
        return chain
    
    async def get_fusion_stats(self) -> Dict[str, Any]:
        """[ğŸ“Š] Get fusion engine statistics"""
        context_window = await self.context_manager.get_context_window()
        
        return {
            "nlp_processor": {
                "intents_supported": len(self.nlp_processor.intent_patterns),
                "entity_types": len(self.nlp_processor.entity_patterns)
            },
            "context_manager": {
                "conversation_length": len(self.context_manager.conversation_history),
                "entities_tracked": len(self.context_manager.entity_memory),
                "unique_entities": sum(len(set(v)) for v in self.context_manager.entity_memory.values())
            },
            "semantic_graph": {
                "nodes": len(self.semantic_graph),
                "edges": sum(len(v) for v in self.semantic_graph.values()) // 2
            },
            "timestamp": datetime.now().isoformat()
        }


# Global fusion engine instance
context_nlp_fusion_engine = ContextNLPFusionEngine()


async def initialize_nlp_fusion():
    """[ğŸš€] Initialize NLP Fusion Engine"""
    nlp_logger.info("[ğŸš€] Initializing Context-NLP Fusion Engine")
    
    # Initialize RAG engine
    from .dag_rag_plus_plus_engine import initialize_rag_engine
    await initialize_rag_engine()
    
    # Initialize ensemble
    from .dag_rag_ensemble_fusion import initialize_ensemble
    await initialize_ensemble()
    
    nlp_logger.info("[âœ…] Context-NLP Fusion Engine initialization complete")


if __name__ == "__main__":
    # Test fusion engine
    async def test_nlp_fusion():
        await initialize_nlp_fusion()
        
        # Test queries
        queries = [
            "How does OSE optimize system performance?",
            "What services are running?",
            "Why is the optimization service slow?",
            "Compare RAG engine with traditional search"
        ]
        
        for query in queries:
            print(f"\n[ğŸ”] Query: {query}")
            result = await context_nlp_fusion_engine.process_query(query)
            
            print(f"[ğŸ“] Response: {result.generated_text}")
            print(f"[ğŸ¯] Intent: {result.parsed_query.intent.value}")
            print(f"[ğŸ“Š] Confidence: {result.confidence:.2f}")
            print(f"[ğŸ·ï¸] Entities: {result.parsed_query.entities}")
            print(f"[ğŸ”—] Relationships: {result.semantic_relationships}")
            print(f"[ğŸ§ ] Reasoning:")
            for step in result.reasoning_chain:
                print(f"     {step}")
        
        stats = await context_nlp_fusion_engine.get_fusion_stats()
        print(f"\n[ğŸ“Š] Fusion Stats: {json.dumps(stats, indent=2)}")
    
    asyncio.run(test_nlp_fusion())
