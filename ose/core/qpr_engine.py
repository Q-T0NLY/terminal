#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë              QUANTUM PATH RECONSTRUCTION (QPR) ENGINE                         ‚ïë
‚ïë           PATH ‚Ä¢ SYMLINK ‚Ä¢ DOTFILE ‚Ä¢ ALIAS Optimizer                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Component B: Comprehensive path/symlink/dotfile/alias reconstruction system
"""

import os
import sys
import json
import shutil
import logging
import subprocess
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, asdict, field
from datetime import datetime
from pathlib import Path
import hashlib
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class PathAnalysis:
    """Results from PATH environment analysis"""
    current_paths: List[str]
    duplicates: List[str]
    non_existent: List[str]
    optimal_order: List[str]
    conflicts: List[Dict]
    recommended_additions: List[str]
    optimization_score: float  # 0-100


@dataclass
class SymlinkAnalysis:
    """Results from symlink analysis"""
    total_scanned: int
    broken_symlinks: List[str]
    circular_symlinks: List[str]
    stale_symlinks: List[str]
    valid_symlinks: List[str]
    reconstruction_plan: List[Dict]


@dataclass
class DotfileAnalysis:
    """Results from dotfile analysis"""
    files_found: List[str]
    conflicts: List[Dict]
    migration_needed: bool
    merge_candidates: List[Tuple[str, str]]
    backup_created: str


@dataclass
class AliasAnalysis:
    """Results from alias analysis"""
    current_aliases: Dict[str, str]
    duplicates: List[str]
    obsolete: List[str]
    suggestions: List[Dict]
    optimization_score: float


@dataclass
class QPRReport:
    """Comprehensive QPR reconstruction report"""
    path_analysis: PathAnalysis
    symlink_analysis: SymlinkAnalysis
    dotfile_analysis: DotfileAnalysis
    alias_analysis: AliasAnalysis
    overall_score: float
    actions_taken: List[str]
    backup_location: str
    timestamp: str
    success: bool
    errors: List[str]


class QuantumPathReconstructor:
    """
    Quantum Path Reconstruction (QPR) Engine
    
    Capabilities:
    - PATH variable optimization and deduplication
    - Broken symlink detection and reconstruction
    - Dotfile migration and conflict resolution
    - Shell alias optimization
    - Atomic operations with full rollback support
    """
    
    # Priority order for PATH optimization
    PATH_PRIORITY = [
        "/usr/local/bin",
        "/opt/homebrew/bin",  # Apple Silicon Homebrew
        "{home}/.local/bin",
        "{home}/bin",
        # Language version managers
        "{home}/.nvm/current/bin",
        "{home}/.pyenv/shims",
        "{home}/.rbenv/shims",
        "{home}/.cargo/bin",
        "{home}/go/bin",
        # System paths
        "/usr/bin",
        "/bin",
        "/usr/sbin",
        "/sbin",
        "/usr/local/sbin",
    ]
    
    # Common symlink directories to scan
    SYMLINK_DIRS = [
        "/usr/local/bin",
        "{home}/.local/bin",
        "{home}/bin",
        "/opt/homebrew/bin",
    ]
    
    # Dotfile mappings (source -> target)
    DOTFILE_MIGRATIONS = {
        '.bashrc': '.zshrc',
        '.bash_profile': '.zprofile',
        '.bash_aliases': '.zsh_aliases',
    }
    
    def __init__(self, dry_run: bool = False):
        """Initialize QPR engine"""
        self.dry_run = dry_run
        self.home = str(Path.home())
        self.backup_dir = None
        self.actions_log = []
        self.errors = []
        
        logger.info(f"üîß QPR Engine initialized (dry_run={dry_run})")
    
    # ==================== BACKUP & ROLLBACK ====================
    
    def create_backup(self) -> str:
        """Create timestamped backup of all configs"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = Path(self.home) / f".qpr_backup_{timestamp}"
        
        if self.dry_run:
            logger.info(f"[DRY RUN] Would create backup: {backup_dir}")
            return str(backup_dir)
        
        backup_dir.mkdir(exist_ok=True)
        
        # Backup dotfiles
        dotfiles = [
            '.zshrc', '.zprofile', '.zshenv', '.zlogin', '.zlogout',
            '.bashrc', '.bash_profile', '.profile',
            '.aliases', '.functions', '.exports'
        ]
        
        for dotfile in dotfiles:
            src = Path(self.home) / dotfile
            if src.exists():
                try:
                    shutil.copy2(src, backup_dir / dotfile)
                    logger.info(f"‚úÖ Backed up: {dotfile}")
                except Exception as e:
                    logger.error(f"‚ùå Backup failed for {dotfile}: {e}")
        
        # Backup PATH
        path_file = backup_dir / "original_path.txt"
        path_file.write_text(os.environ.get('PATH', ''))
        
        self.backup_dir = str(backup_dir)
        logger.info(f"üì¶ Backup created: {backup_dir}")
        return str(backup_dir)
    
    def rollback(self):
        """Rollback to backup"""
        if not self.backup_dir or not os.path.exists(self.backup_dir):
            logger.error("‚ùå No backup found for rollback")
            return False
        
        if self.dry_run:
            logger.info(f"[DRY RUN] Would rollback from: {self.backup_dir}")
            return True
        
        backup_path = Path(self.backup_dir)
        for backup_file in backup_path.iterdir():
            if backup_file.name == "original_path.txt":
                continue
            
            target = Path(self.home) / backup_file.name
            try:
                shutil.copy2(backup_file, target)
                logger.info(f"‚úÖ Restored: {backup_file.name}")
            except Exception as e:
                logger.error(f"‚ùå Restore failed for {backup_file.name}: {e}")
        
        logger.info("üîÑ Rollback complete")
        return True
    
    # ==================== PATH ANALYSIS & RECONSTRUCTION ====================
    
    def analyze_path(self) -> PathAnalysis:
        """Analyze PATH environment variable"""
        logger.info("üîç Analyzing PATH...")
        
        current_path = os.environ.get('PATH', '').split(':')
        current_path = [p for p in current_path if p]  # Remove empty
        
        # Find duplicates
        seen = set()
        duplicates = []
        for p in current_path:
            if p in seen:
                duplicates.append(p)
            seen.add(p)
        
        # Find non-existent paths
        non_existent = [p for p in current_path if not os.path.exists(p)]
        
        # Calculate optimal order
        optimal_order = self._calculate_optimal_path_order(current_path)
        
        # Detect conflicts (multiple package managers)
        conflicts = self._detect_path_conflicts(current_path)
        
        # Suggest additions
        recommended = self._suggest_path_additions()
        
        # Calculate optimization score
        optimization_score = self._calculate_path_score(
            len(duplicates), len(non_existent), len(conflicts)
        )
        
        return PathAnalysis(
            current_paths=current_path,
            duplicates=duplicates,
            non_existent=non_existent,
            optimal_order=optimal_order,
            conflicts=conflicts,
            recommended_additions=recommended,
            optimization_score=optimization_score
        )
    
    def _calculate_optimal_path_order(self, current_paths: List[str]) -> List[str]:
        """Calculate optimal PATH ordering"""
        optimal = []
        remaining = current_paths.copy()
        
        # Expand home directory in priority paths
        priority_expanded = [
            p.format(home=self.home) for p in self.PATH_PRIORITY
        ]
        
        # Add priority paths first (if they exist in current PATH)
        for priority_path in priority_expanded:
            if priority_path in remaining:
                optimal.append(priority_path)
                remaining.remove(priority_path)
        
        # Add remaining paths that exist
        for path in remaining:
            if os.path.exists(path) and path not in optimal:
                optimal.append(path)
        
        return optimal
    
    def _detect_path_conflicts(self, paths: List[str]) -> List[Dict]:
        """Detect package manager conflicts"""
        conflicts = []
        
        # Check for Homebrew vs MacPorts
        has_homebrew = any('/brew' in p or '/usr/local/bin' in p for p in paths)
        has_macports = any('/opt/local' in p or 'macports' in p.lower() for p in paths)
        
        if has_homebrew and has_macports:
            conflicts.append({
                'type': 'package_manager',
                'severity': 'high',
                'description': 'Both Homebrew and MacPorts detected - may cause conflicts',
                'recommendation': 'Choose one primary package manager'
            })
        
        # Check for multiple Python paths
        python_paths = [p for p in paths if 'python' in p.lower() or 'pyenv' in p]
        if len(python_paths) > 2:
            conflicts.append({
                'type': 'python_environment',
                'severity': 'medium',
                'description': f'Multiple Python paths found: {len(python_paths)}',
                'recommendation': 'Consolidate Python installations'
            })
        
        return conflicts
    
    def _suggest_path_additions(self) -> List[str]:
        """Suggest PATH additions based on installed software"""
        suggestions = []
        
        # Check for common tools
        checks = {
            f"{self.home}/.local/bin": "User local binaries",
            f"{self.home}/bin": "User bin directory",
            "/usr/local/sbin": "System administration binaries",
        }
        
        current_path_set = set(os.environ.get('PATH', '').split(':'))
        
        for path, description in checks.items():
            if os.path.exists(path) and path not in current_path_set:
                suggestions.append(path)
        
        return suggestions
    
    def _calculate_path_score(self, duplicates: int, non_existent: int, 
                             conflicts: int) -> float:
        """Calculate PATH optimization score"""
        base_score = 100.0
        base_score -= (duplicates * 5)  # -5 per duplicate
        base_score -= (non_existent * 10)  # -10 per non-existent
        base_score -= (conflicts * 15)  # -15 per conflict
        
        return max(0.0, min(100.0, base_score))
    
    def reconstruct_path(self, analysis: PathAnalysis) -> bool:
        """Apply optimized PATH to shell configs"""
        optimal_path = ':'.join(analysis.optimal_order)
        
        if self.dry_run:
            logger.info(f"[DRY RUN] Would set PATH to: {optimal_path}")
            return True
        
        # Update .zshrc
        zshrc = Path(self.home) / '.zshrc'
        if zshrc.exists():
            self._update_path_in_file(zshrc, optimal_path)
        
        # Update .zprofile
        zprofile = Path(self.home) / '.zprofile'
        if zprofile.exists():
            self._update_path_in_file(zprofile, optimal_path)
        
        self.actions_log.append(f"Updated PATH in shell configs")
        logger.info("‚úÖ PATH reconstructed")
        return True
    
    def _update_path_in_file(self, file_path: Path, new_path: str):
        """Update PATH export in a shell config file"""
        if not file_path.exists():
            return
        
        content = file_path.read_text()
        
        # Remove existing PATH exports
        lines = content.split('\n')
        new_lines = []
        for line in lines:
            if not (line.strip().startswith('export PATH=') or 
                   line.strip().startswith('PATH=')):
                new_lines.append(line)
        
        # Add new PATH export
        new_lines.append(f'\n# QPR Optimized PATH - {datetime.now().isoformat()}')
        new_lines.append(f'export PATH="{new_path}"')
        
        file_path.write_text('\n'.join(new_lines))
    
    # ==================== SYMLINK ANALYSIS & RECONSTRUCTION ====================
    
    def analyze_symlinks(self) -> SymlinkAnalysis:
        """Analyze symlinks in common directories"""
        logger.info("üîó Analyzing symlinks...")
        
        broken = []
        circular = []
        stale = []
        valid = []
        total = 0
        
        scan_dirs = [d.format(home=self.home) for d in self.SYMLINK_DIRS]
        
        for scan_dir in scan_dirs:
            if not os.path.exists(scan_dir):
                continue
            
            for item in Path(scan_dir).iterdir():
                if not item.is_symlink():
                    continue
                
                total += 1
                
                try:
                    target = item.resolve(strict=False)
                    
                    # Check if broken
                    if not item.exists():
                        broken.append(str(item))
                    # Check if circular
                    elif self._is_circular_symlink(item):
                        circular.append(str(item))
                    # Check if stale
                    elif self._is_stale_symlink(item):
                        stale.append(str(item))
                    else:
                        valid.append(str(item))
                except Exception as e:
                    logger.warning(f"Error analyzing {item}: {e}")
                    broken.append(str(item))
        
        # Generate reconstruction plan
        reconstruction_plan = self._generate_symlink_plan(broken, circular, stale)
        
        return SymlinkAnalysis(
            total_scanned=total,
            broken_symlinks=broken,
            circular_symlinks=circular,
            stale_symlinks=stale,
            valid_symlinks=valid,
            reconstruction_plan=reconstruction_plan
        )
    
    def _is_circular_symlink(self, link: Path) -> bool:
        """Check if symlink is circular"""
        try:
            visited = set()
            current = link
            
            while current.is_symlink():
                if str(current) in visited:
                    return True
                visited.add(str(current))
                current = current.resolve(strict=False)
            
            return False
        except:
            return False
    
    def _is_stale_symlink(self, link: Path) -> bool:
        """Check if symlink points to old version"""
        try:
            target = link.resolve()
            target_str = str(target)
            
            # Check for version numbers in path
            if re.search(r'/\d+\.\d+', target_str):
                # Check if newer version exists
                parent = target.parent.parent
                if parent.exists():
                    versions = [d for d in parent.iterdir() if d.is_dir()]
                    if len(versions) > 1:
                        return True
            
            return False
        except:
            return False
    
    def _generate_symlink_plan(self, broken: List[str], circular: List[str], 
                               stale: List[str]) -> List[Dict]:
        """Generate reconstruction plan for symlinks"""
        plan = []
        
        for link in broken:
            plan.append({
                'action': 'remove',
                'target': link,
                'reason': 'broken symlink'
            })
        
        for link in circular:
            plan.append({
                'action': 'remove',
                'target': link,
                'reason': 'circular reference'
            })
        
        for link in stale:
            plan.append({
                'action': 'update',
                'target': link,
                'reason': 'stale version'
            })
        
        return plan
    
    def reconstruct_symlinks(self, analysis: SymlinkAnalysis) -> bool:
        """Execute symlink reconstruction plan"""
        if self.dry_run:
            logger.info(f"[DRY RUN] Would process {len(analysis.reconstruction_plan)} symlinks")
            return True
        
        for action_item in analysis.reconstruction_plan:
            action = action_item['action']
            target = action_item['target']
            
            try:
                if action == 'remove':
                    Path(target).unlink(missing_ok=True)
                    logger.info(f"‚úÖ Removed: {target}")
                    self.actions_log.append(f"Removed symlink: {target}")
                
                elif action == 'update':
                    # Would need logic to find correct target
                    logger.info(f"‚ö†Ô∏è Manual update needed: {target}")
            
            except Exception as e:
                logger.error(f"‚ùå Failed to process {target}: {e}")
                self.errors.append(f"Symlink error: {target} - {e}")
        
        logger.info("‚úÖ Symlink reconstruction complete")
        return True
    
    # ==================== DOTFILE MIGRATION ====================
    
    def analyze_dotfiles(self) -> DotfileAnalysis:
        """Analyze dotfiles for migration needs"""
        logger.info("üìÑ Analyzing dotfiles...")
        
        files_found = []
        conflicts = []
        merge_candidates = []
        
        # Check which dotfiles exist
        for source, target in self.DOTFILE_MIGRATIONS.items():
            source_path = Path(self.home) / source
            target_path = Path(self.home) / target
            
            if source_path.exists():
                files_found.append(source)
                
                if target_path.exists():
                    # Both exist - check for conflicts
                    conflict_items = self._compare_dotfiles(source_path, target_path)
                    if conflict_items:
                        conflicts.extend(conflict_items)
                else:
                    # Source exists but target doesn't - candidate for migration
                    merge_candidates.append((source, target))
        
        migration_needed = len(merge_candidates) > 0 or len(conflicts) > 0
        backup_created = self.backup_dir or "Not created yet"
        
        return DotfileAnalysis(
            files_found=files_found,
            conflicts=conflicts,
            migration_needed=migration_needed,
            merge_candidates=merge_candidates,
            backup_created=backup_created
        )
    
    def _compare_dotfiles(self, source: Path, target: Path) -> List[Dict]:
        """Compare two dotfiles for conflicts"""
        conflicts = []
        
        try:
            source_aliases = self._extract_aliases(source)
            target_aliases = self._extract_aliases(target)
            
            for alias_name, source_value in source_aliases.items():
                if alias_name in target_aliases:
                    target_value = target_aliases[alias_name]
                    if source_value != target_value:
                        conflicts.append({
                            'type': 'alias',
                            'name': alias_name,
                            'source_file': str(source),
                            'source_value': source_value,
                            'target_file': str(target),
                            'target_value': target_value
                        })
        except Exception as e:
            logger.error(f"Error comparing dotfiles: {e}")
        
        return conflicts
    
    def _extract_aliases(self, file_path: Path) -> Dict[str, str]:
        """Extract aliases from a shell config file"""
        aliases = {}
        
        if not file_path.exists():
            return aliases
        
        try:
            content = file_path.read_text()
            for line in content.split('\n'):
                line = line.strip()
                if line.startswith('alias '):
                    # Parse: alias name='value'
                    match = re.match(r'alias\s+(\w+)=(.*)', line)
                    if match:
                        name = match.group(1)
                        value = match.group(2).strip('\'"')
                        aliases[name] = value
        except Exception as e:
            logger.error(f"Error extracting aliases from {file_path}: {e}")
        
        return aliases
    
    def migrate_dotfiles(self, analysis: DotfileAnalysis) -> bool:
        """Migrate dotfiles intelligently"""
        if not analysis.migration_needed:
            logger.info("‚úÖ No dotfile migration needed")
            return True
        
        if self.dry_run:
            logger.info(f"[DRY RUN] Would migrate {len(analysis.merge_candidates)} files")
            return True
        
        for source, target in analysis.merge_candidates:
            source_path = Path(self.home) / source
            target_path = Path(self.home) / target
            
            try:
                if not target_path.exists():
                    # Simple copy
                    shutil.copy2(source_path, target_path)
                    logger.info(f"‚úÖ Migrated: {source} ‚Üí {target}")
                    self.actions_log.append(f"Migrated {source} to {target}")
                else:
                    # Merge with conflict resolution
                    self._merge_dotfiles(source_path, target_path)
                    logger.info(f"‚úÖ Merged: {source} ‚Üí {target}")
                    self.actions_log.append(f"Merged {source} into {target}")
            
            except Exception as e:
                logger.error(f"‚ùå Migration failed for {source}: {e}")
                self.errors.append(f"Dotfile migration error: {source} - {e}")
        
        logger.info("‚úÖ Dotfile migration complete")
        return True
    
    def _merge_dotfiles(self, source: Path, target: Path):
        """Merge source dotfile into target"""
        source_content = source.read_text()
        target_content = target.read_text()
        
        # Add migration header
        merged = target_content + f"\n\n# Migrated from {source.name} - {datetime.now().isoformat()}\n"
        merged += source_content
        
        target.write_text(merged)
    
    # ==================== ALIAS OPTIMIZATION ====================
    
    def analyze_aliases(self) -> AliasAnalysis:
        """Analyze shell aliases"""
        logger.info("üéØ Analyzing aliases...")
        
        zshrc = Path(self.home) / '.zshrc'
        aliases = self._extract_aliases(zshrc)
        
        # Find duplicates (same value)
        duplicates = self._find_duplicate_aliases(aliases)
        
        # Find obsolete (point to non-existent commands)
        obsolete = self._find_obsolete_aliases(aliases)
        
        # Generate suggestions
        suggestions = self._suggest_aliases()
        
        # Calculate score
        optimization_score = 100.0 - (len(duplicates) * 5) - (len(obsolete) * 10)
        optimization_score = max(0.0, min(100.0, optimization_score))
        
        return AliasAnalysis(
            current_aliases=aliases,
            duplicates=duplicates,
            obsolete=obsolete,
            suggestions=suggestions,
            optimization_score=optimization_score
        )
    
    def _find_duplicate_aliases(self, aliases: Dict[str, str]) -> List[str]:
        """Find duplicate alias values"""
        value_to_names = {}
        for name, value in aliases.items():
            if value not in value_to_names:
                value_to_names[value] = []
            value_to_names[value].append(name)
        
        duplicates = []
        for value, names in value_to_names.items():
            if len(names) > 1:
                duplicates.extend(names[1:])  # Keep first, mark others as duplicates
        
        return duplicates
    
    def _find_obsolete_aliases(self, aliases: Dict[str, str]) -> List[str]:
        """Find obsolete aliases"""
        obsolete = []
        
        for name, value in aliases.items():
            # Extract command (first word)
            command = value.split()[0] if value else ''
            
            # Check if command exists
            if command and not shutil.which(command):
                obsolete.append(name)
        
        return obsolete
    
    def _suggest_aliases(self) -> List[Dict]:
        """Suggest useful aliases"""
        suggestions = [
            {'name': 'll', 'value': 'ls -lah', 'description': 'Detailed list'},
            {'name': 'la', 'value': 'ls -A', 'description': 'List all'},
            {'name': 'gs', 'value': 'git status', 'description': 'Git status'},
            {'name': 'gp', 'value': 'git pull', 'description': 'Git pull'},
            {'name': '..', 'value': 'cd ..', 'description': 'Up one directory'},
            {'name': '...', 'value': 'cd ../..', 'description': 'Up two directories'},
            {'name': 'mkd', 'value': 'mkdir -p', 'description': 'Make directory with parents'},
        ]
        
        return suggestions
    
    def optimize_aliases(self, analysis: AliasAnalysis) -> bool:
        """Optimize alias configuration"""
        if self.dry_run:
            logger.info(f"[DRY RUN] Would optimize {len(analysis.current_aliases)} aliases")
            return True
        
        zshrc = Path(self.home) / '.zshrc'
        if not zshrc.exists():
            logger.warning("‚ö†Ô∏è No .zshrc found")
            return False
        
        # Remove duplicates and obsolete
        content = zshrc.read_text()
        lines = content.split('\n')
        new_lines = []
        
        for line in lines:
            # Remove obsolete and duplicate aliases
            skip = False
            for obsolete in analysis.obsolete:
                if f'alias {obsolete}=' in line:
                    skip = True
                    break
            for duplicate in analysis.duplicates:
                if f'alias {duplicate}=' in line:
                    skip = True
                    break
            
            if not skip:
                new_lines.append(line)
        
        # Add suggestions
        new_lines.append(f'\n# QPR Suggested Aliases - {datetime.now().isoformat()}')
        for suggestion in analysis.suggestions:
            alias_line = f"alias {suggestion['name']}='{suggestion['value']}'  # {suggestion['description']}"
            if alias_line not in '\n'.join(new_lines):
                new_lines.append(alias_line)
        
        zshrc.write_text('\n'.join(new_lines))
        
        logger.info("‚úÖ Alias optimization complete")
        self.actions_log.append("Optimized shell aliases")
        return True
    
    # ==================== MAIN ORCHESTRATION ====================
    
    def run_full_reconstruction(self) -> QPRReport:
        """Run complete QPR reconstruction process"""
        logger.info("üöÄ Starting full QPR reconstruction...")
        
        try:
            # Create backup
            backup_location = self.create_backup()
            
            # Run all analyses
            path_analysis = self.analyze_path()
            symlink_analysis = self.analyze_symlinks()
            dotfile_analysis = self.analyze_dotfiles()
            alias_analysis = self.analyze_aliases()
            
            # Execute reconstructions
            self.reconstruct_path(path_analysis)
            self.reconstruct_symlinks(symlink_analysis)
            self.migrate_dotfiles(dotfile_analysis)
            self.optimize_aliases(alias_analysis)
            
            # Calculate overall score
            overall_score = (
                path_analysis.optimization_score * 0.30 +
                alias_analysis.optimization_score * 0.20 +
                (100.0 if not dotfile_analysis.conflicts else 80.0) * 0.25 +
                (100.0 - len(symlink_analysis.broken_symlinks) * 5) * 0.25
            )
            overall_score = max(0.0, min(100.0, overall_score))
            
            # Generate report
            report = QPRReport(
                path_analysis=path_analysis,
                symlink_analysis=symlink_analysis,
                dotfile_analysis=dotfile_analysis,
                alias_analysis=alias_analysis,
                overall_score=round(overall_score, 2),
                actions_taken=self.actions_log,
                backup_location=backup_location,
                timestamp=datetime.now().isoformat(),
                success=len(self.errors) == 0,
                errors=self.errors
            )
            
            logger.info(f"‚úÖ QPR reconstruction complete! Score: {report.overall_score}/100")
            return report
            
        except Exception as e:
            logger.error(f"‚ùå QPR reconstruction failed: {e}")
            self.errors.append(str(e))
            
            # Attempt rollback
            if self.backup_dir:
                logger.info("üîÑ Attempting rollback...")
                self.rollback()
            
            raise


def main():
    """CLI interface for testing"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Quantum Path Reconstruction Engine')
    parser.add_argument('--dry-run', action='store_true', help='Simulate without making changes')
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("üîß QUANTUM PATH RECONSTRUCTOR - System Optimization")
    print("="*80 + "\n")
    
    qpr = QuantumPathReconstructor(dry_run=args.dry_run)
    report = qpr.run_full_reconstruction()
    
    print(f"\nüìä Overall Score: {report.overall_score:.1f}/100")
    print(f"‚úÖ Success: {report.success}")
    print(f"üì¶ Backup: {report.backup_location}")
    
    print(f"\nüìà Component Scores:")
    print(f"  ‚Ä¢ PATH:     {report.path_analysis.optimization_score:.1f}/100")
    print(f"  ‚Ä¢ Aliases:  {report.alias_analysis.optimization_score:.1f}/100")
    print(f"  ‚Ä¢ Symlinks: {len(report.symlink_analysis.valid_symlinks)} valid, "
          f"{len(report.symlink_analysis.broken_symlinks)} broken")
    print(f"  ‚Ä¢ Dotfiles: {len(report.dotfile_analysis.files_found)} found")
    
    print(f"\nüî® Actions Taken:")
    for action in report.actions_taken:
        print(f"  ‚Ä¢ {action}")
    
    if report.errors:
        print(f"\n‚ùå Errors:")
        for error in report.errors:
            print(f"  ‚Ä¢ {error}")
    
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
